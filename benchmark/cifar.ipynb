{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64a63148",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:52:57.966617Z",
     "iopub.status.busy": "2025-06-07T11:52:57.966361Z",
     "iopub.status.idle": "2025-06-07T11:53:08.674270Z",
     "shell.execute_reply": "2025-06-07T11:53:08.673331Z"
    },
    "papermill": {
     "duration": 10.713562,
     "end_time": "2025-06-07T11:53:08.676223",
     "exception": false,
     "start_time": "2025-06-07T11:52:57.962661",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n",
      "  Building wheel for actix (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\r\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/EmotionEngineer/actix.git -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dbff9dc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:53:08.682037Z",
     "iopub.status.busy": "2025-06-07T11:53:08.681797Z",
     "iopub.status.idle": "2025-06-07T11:53:26.267985Z",
     "shell.execute_reply": "2025-06-07T11:53:26.267222Z"
    },
    "papermill": {
     "duration": 17.590608,
     "end_time": "2025-06-07T11:53:26.269522",
     "exception": false,
     "start_time": "2025-06-07T11:53:08.678914",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-07 11:53:10.912594: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1749297191.177106      19 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1749297191.255009      19 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "# --- 1. Import Necessary Libraries ---\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import (Layer, Dense, BatchNormalization, Activation, Input, \n",
    "                                     Conv2D, MaxPooling2D, Flatten)\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Suppress warnings for a cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=UserWarning, module='tensorflow')\n",
    "warnings.filterwarnings('ignore', category=FutureWarning, module='tensorflow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b517c6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:53:26.275563Z",
     "iopub.status.busy": "2025-06-07T11:53:26.274956Z",
     "iopub.status.idle": "2025-06-07T11:53:31.108895Z",
     "shell.execute_reply": "2025-06-07T11:53:31.108121Z"
    },
    "papermill": {
     "duration": 4.838084,
     "end_time": "2025-06-07T11:53:31.110039",
     "exception": false,
     "start_time": "2025-06-07T11:53:26.271955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading activation functions from actix...\n",
      "Loading complete.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- 2. Import and Setup Activations from Actix ---\n",
    "try:\n",
    "    import actix\n",
    "except ImportError:\n",
    "    print(\"Error: 'actix' library is not installed.\")\n",
    "    print(\"Please install it using the command: pip install git+https://github.com/EmotionEngineer/actix.git\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# List of activation function names from actix to be tested\n",
    "ACTIX_FUNCTION_NAMES = [\n",
    "    \"AdaptiveHyperbolicLogarithm\", \"ParametricGeneralizedGompertzActivation\",\n",
    "    \"ComplexHarmonicActivation\", \"WeibullSoftplusActivation\", \"AdaptiveErfSwish\",\n",
    "    \"ParametricBetaSoftsign\", \"ParametricArcSinhGate\", \"GeneralizedAlphaSigmoid\",\n",
    "    \"RiemannianSoftsignActivation\", \"QuantumTanhActivation\", \"BipolarGaussianArctanActivation\",\n",
    "    \"EllipticGaussianActivation\", \"ExpArcTanHarmonicActivation\", \"ParametricLogish\",\n",
    "    \"A_ELuC\", \"OptimA\", \"ParametricSmoothStep\", \"OptimXTemporal\", \"ExpoSoft\", \"UnifiedSineExp\"\n",
    "]\n",
    "\n",
    "# Dynamically create the activation map from the actix library\n",
    "CUSTOM_ACTIVATIONS_MAP = {}\n",
    "print(\"Loading activation functions from actix...\")\n",
    "for name in ACTIX_FUNCTION_NAMES:\n",
    "    try:\n",
    "        activation_class = getattr(actix, name)\n",
    "        CUSTOM_ACTIVATIONS_MAP[name] = activation_class\n",
    "    except AttributeError:\n",
    "        print(f\"  - WARNING: Activation function '{name}' not found in the actix library and will be skipped.\")\n",
    "print(\"Loading complete.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0824c97e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:53:31.115472Z",
     "iopub.status.busy": "2025-06-07T11:53:31.115257Z",
     "iopub.status.idle": "2025-06-07T11:53:31.118831Z",
     "shell.execute_reply": "2025-06-07T11:53:31.118323Z"
    },
    "papermill": {
     "duration": 0.007465,
     "end_time": "2025-06-07T11:53:31.119886",
     "exception": false,
     "start_time": "2025-06-07T11:53:31.112421",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 3. Constants and Experiment Configuration ---\n",
    "NUM_SEEDS = 3        # Reduced for faster execution, as CIFAR-10 training takes longer\n",
    "EPOCHS = 150\n",
    "PATIENCE = 20        # More aggressive early stopping\n",
    "BATCH_SIZE = 64      # Increased for more stable training on images\n",
    "LEARNING_RATE = 1e-3 # A standard learning rate for Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78d3cd4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:53:31.124772Z",
     "iopub.status.busy": "2025-06-07T11:53:31.124398Z",
     "iopub.status.idle": "2025-06-07T11:53:31.129189Z",
     "shell.execute_reply": "2025-06-07T11:53:31.128642Z"
    },
    "papermill": {
     "duration": 0.008402,
     "end_time": "2025-06-07T11:53:31.130322",
     "exception": false,
     "start_time": "2025-06-07T11:53:31.121920",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 4. Layer Naming Utilities ---\n",
    "_layer_name_counters = {}\n",
    "\n",
    "def _get_unique_layer_name(base_name_key):\n",
    "    if base_name_key not in _layer_name_counters:\n",
    "        _layer_name_counters[base_name_key] = 0\n",
    "    _layer_name_counters[base_name_key] += 1\n",
    "    return f\"{base_name_key}_{_layer_name_counters[base_name_key]}\"\n",
    "\n",
    "def _reset_layer_name_counters():\n",
    "    global _layer_name_counters\n",
    "    _layer_name_counters = {}\n",
    "\n",
    "def _add_activation_layer(model, activation_name_str):\n",
    "    unique_name = _get_unique_layer_name(activation_name_str.lower().replace(\" \", \"_\").replace(\"-\",\"_\"))\n",
    "    if activation_name_str in CUSTOM_ACTIVATIONS_MAP:\n",
    "        # Use a custom activation from actix\n",
    "        model.add(CUSTOM_ACTIVATIONS_MAP[activation_name_str](name=unique_name))\n",
    "    else:\n",
    "        # Use a standard TensorFlow activation\n",
    "        model.add(Activation(activation_name_str, name=unique_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1469b21e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:53:31.135439Z",
     "iopub.status.busy": "2025-06-07T11:53:31.135221Z",
     "iopub.status.idle": "2025-06-07T11:53:31.142296Z",
     "shell.execute_reply": "2025-06-07T11:53:31.141517Z"
    },
    "papermill": {
     "duration": 0.011027,
     "end_time": "2025-06-07T11:53:31.143491",
     "exception": false,
     "start_time": "2025-06-07T11:53:31.132464",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 5. Model Definition for Classification Task (CNN) ---\n",
    "def create_cifar10_cnn_model(input_shape, activation_name_str, num_classes=10):\n",
    "    model_name_prefix = activation_name_str.replace(' ','_').replace(\"-\",\"_\").lower()\n",
    "    model = Sequential(name=f\"cifar10_cnn_model_{model_name_prefix}\")\n",
    "\n",
    "    model.add(Input(shape=input_shape, name=_get_unique_layer_name(\"input_layer\")))\n",
    "\n",
    "    # Block 1\n",
    "    model.add(Conv2D(32, (3, 3), padding='same', name=_get_unique_layer_name(\"conv2d\")))\n",
    "    _add_activation_layer(model, activation_name_str)\n",
    "    model.add(BatchNormalization(name=_get_unique_layer_name(\"bn\")))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name=_get_unique_layer_name(\"maxpool\")))\n",
    "\n",
    "    # Block 2\n",
    "    model.add(Conv2D(64, (3, 3), padding='same', name=_get_unique_layer_name(\"conv2d\")))\n",
    "    _add_activation_layer(model, activation_name_str)\n",
    "    model.add(BatchNormalization(name=_get_unique_layer_name(\"bn\")))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2), name=_get_unique_layer_name(\"maxpool\")))\n",
    "    \n",
    "    # Block 3\n",
    "    model.add(Conv2D(128, (3, 3), padding='same', name=_get_unique_layer_name(\"conv2d\")))\n",
    "    _add_activation_layer(model, activation_name_str)\n",
    "    model.add(BatchNormalization(name=_get_unique_layer_name(\"bn\")))\n",
    "\n",
    "    # Classifier Head\n",
    "    model.add(Flatten(name=_get_unique_layer_name(\"flatten\")))\n",
    "    model.add(Dense(128, name=_get_unique_layer_name(\"dense\")))\n",
    "    _add_activation_layer(model, activation_name_str)\n",
    "    model.add(BatchNormalization(name=_get_unique_layer_name(\"bn\")))\n",
    "    \n",
    "    model.add(Dense(num_classes, activation='softmax', name=_get_unique_layer_name(\"output_dense\")))\n",
    "\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "    model.compile(optimizer=optimizer,\n",
    "                  loss='categorical_crossentropy', # Loss function for multi-class classification\n",
    "                  metrics=['accuracy'])            # Primary metric is accuracy\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f84c46f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:53:31.148302Z",
     "iopub.status.busy": "2025-06-07T11:53:31.148046Z",
     "iopub.status.idle": "2025-06-07T11:53:31.152156Z",
     "shell.execute_reply": "2025-06-07T11:53:31.151455Z"
    },
    "papermill": {
     "duration": 0.007704,
     "end_time": "2025-06-07T11:53:31.153272",
     "exception": false,
     "start_time": "2025-06-07T11:53:31.145568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# --- 6. Data Loading and Preprocessing for CIFAR-10 ---\n",
    "def load_and_preprocess_cifar10():\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "    # Normalize images: scale pixel values to the [0, 1] range\n",
    "    x_train = x_train.astype('float32') / 255.0\n",
    "    x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "    # Convert labels to one-hot encoding\n",
    "    num_classes = 10\n",
    "    y_train = to_categorical(y_train, num_classes)\n",
    "    y_test = to_categorical(y_test, num_classes)\n",
    "\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf320ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-07T11:53:31.158318Z",
     "iopub.status.busy": "2025-06-07T11:53:31.158098Z",
     "iopub.status.idle": "2025-06-07T17:58:06.346142Z",
     "shell.execute_reply": "2025-06-07T17:58:06.345371Z"
    },
    "papermill": {
     "duration": 21875.201031,
     "end_time": "2025-06-07T17:58:06.356397",
     "exception": false,
     "start_time": "2025-06-07T11:53:31.155366",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 2.18.0\n",
      "Num GPUs Available: 1, Memory growth enabled.\n",
      "TensorFlow is using GPU.\n",
      "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
      "\u001b[1m170498071/170498071\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 0us/step\n",
      "\n",
      "\n",
      "--- Benchmarking on Dataset: CIFAR-10 ---\n",
      "\n",
      "  --- Testing Activation: relu ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1749297220.484765      19 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15513 MB memory:  -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Seed 1/3 (Actual seed: 42) for relu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1749297227.975798      95 service.cc:148] XLA service 0x7e19fc0122f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1749297227.976749      95 service.cc:156]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\n",
      "I0000 00:00:1749297228.431757      95 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "I0000 00:00:1749297231.176367      95 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Seed 1 Loss: 1.8326, Accuracy: 0.7595\n",
      "    Seed 2/3 (Actual seed: 43) for relu\n",
      "      Seed 2 Loss: 2.5055, Accuracy: 0.7725\n",
      "    Seed 3/3 (Actual seed: 44) for relu\n",
      "      Seed 3 Loss: 2.4490, Accuracy: 0.7719\n",
      "    Results for relu: Mean Loss: 2.2624, Mean Accuracy: 0.7680\n",
      "\n",
      "  --- Testing Activation: gelu ---\n",
      "    Seed 1/3 (Actual seed: 42) for gelu\n",
      "      Seed 1 Loss: 2.5398, Accuracy: 0.7704\n",
      "    Seed 2/3 (Actual seed: 43) for gelu\n",
      "      Seed 2 Loss: 2.0422, Accuracy: 0.7655\n",
      "    Seed 3/3 (Actual seed: 44) for gelu\n",
      "      Seed 3 Loss: 2.1219, Accuracy: 0.7751\n",
      "    Results for gelu: Mean Loss: 2.2346, Mean Accuracy: 0.7703\n",
      "\n",
      "  --- Testing Activation: swish ---\n",
      "    Seed 1/3 (Actual seed: 42) for swish\n",
      "      Seed 1 Loss: 1.9723, Accuracy: 0.7724\n",
      "    Seed 2/3 (Actual seed: 43) for swish\n",
      "      Seed 2 Loss: 2.3744, Accuracy: 0.7632\n",
      "    Seed 3/3 (Actual seed: 44) for swish\n",
      "      Seed 3 Loss: 2.2253, Accuracy: 0.7736\n",
      "    Results for swish: Mean Loss: 2.1907, Mean Accuracy: 0.7697\n",
      "\n",
      "  --- Testing Activation: mish ---\n",
      "    Seed 1/3 (Actual seed: 42) for mish\n",
      "      Seed 1 Loss: 2.3532, Accuracy: 0.7744\n",
      "    Seed 2/3 (Actual seed: 43) for mish\n",
      "      Seed 2 Loss: 1.7117, Accuracy: 0.7460\n",
      "    Seed 3/3 (Actual seed: 44) for mish\n",
      "      Seed 3 Loss: 2.3865, Accuracy: 0.7711\n",
      "    Results for mish: Mean Loss: 2.1505, Mean Accuracy: 0.7638\n",
      "\n",
      "  --- Testing Activation: sigmoid ---\n",
      "    Seed 1/3 (Actual seed: 42) for sigmoid\n",
      "      Seed 1 Loss: 1.9964, Accuracy: 0.7416\n",
      "    Seed 2/3 (Actual seed: 43) for sigmoid\n",
      "      Seed 2 Loss: 2.0050, Accuracy: 0.7374\n",
      "    Seed 3/3 (Actual seed: 44) for sigmoid\n",
      "      Seed 3 Loss: 1.1596, Accuracy: 0.7078\n",
      "    Results for sigmoid: Mean Loss: 1.7203, Mean Accuracy: 0.7289\n",
      "\n",
      "  --- Testing Activation: AdaptiveHyperbolicLogarithm ---\n",
      "    Seed 1/3 (Actual seed: 42) for AdaptiveHyperbolicLogarithm\n",
      "      Seed 1 Loss: 2.0760, Accuracy: 0.7580\n",
      "    Seed 2/3 (Actual seed: 43) for AdaptiveHyperbolicLogarithm\n",
      "      Seed 2 Loss: 1.7141, Accuracy: 0.7475\n",
      "    Seed 3/3 (Actual seed: 44) for AdaptiveHyperbolicLogarithm\n",
      "      Seed 3 Loss: 1.9774, Accuracy: 0.7452\n",
      "    Results for AdaptiveHyperbolicLogarithm: Mean Loss: 1.9225, Mean Accuracy: 0.7502\n",
      "\n",
      "  --- Testing Activation: ParametricGeneralizedGompertzActivation ---\n",
      "    Seed 1/3 (Actual seed: 42) for ParametricGeneralizedGompertzActivation\n",
      "      Seed 1 Loss: 2.0351, Accuracy: 0.7444\n",
      "    Seed 2/3 (Actual seed: 43) for ParametricGeneralizedGompertzActivation\n",
      "      Seed 2 Loss: 2.2327, Accuracy: 0.7615\n",
      "    Seed 3/3 (Actual seed: 44) for ParametricGeneralizedGompertzActivation\n",
      "      Seed 3 Loss: 2.5804, Accuracy: 0.7546\n",
      "    Results for ParametricGeneralizedGompertzActivation: Mean Loss: 2.2827, Mean Accuracy: 0.7535\n",
      "\n",
      "  --- Testing Activation: ComplexHarmonicActivation ---\n",
      "    Seed 1/3 (Actual seed: 42) for ComplexHarmonicActivation\n",
      "      Seed 1 Loss: 1.4916, Accuracy: 0.4672\n",
      "    Seed 2/3 (Actual seed: 43) for ComplexHarmonicActivation\n",
      "      Seed 2 Loss: 1.4347, Accuracy: 0.4915\n",
      "    Seed 3/3 (Actual seed: 44) for ComplexHarmonicActivation\n",
      "      Seed 3 Loss: 1.4748, Accuracy: 0.4635\n",
      "    Results for ComplexHarmonicActivation: Mean Loss: 1.4670, Mean Accuracy: 0.4741\n",
      "\n",
      "  --- Testing Activation: WeibullSoftplusActivation ---\n",
      "    Seed 1/3 (Actual seed: 42) for WeibullSoftplusActivation\n",
      "      Seed 1 Loss: 1.9603, Accuracy: 0.7669\n",
      "    Seed 2/3 (Actual seed: 43) for WeibullSoftplusActivation\n",
      "      Seed 2 Loss: 1.8273, Accuracy: 0.7552\n",
      "    Seed 3/3 (Actual seed: 44) for WeibullSoftplusActivation\n",
      "      Seed 3 Loss: 2.3632, Accuracy: 0.7712\n",
      "    Results for WeibullSoftplusActivation: Mean Loss: 2.0503, Mean Accuracy: 0.7644\n",
      "\n",
      "  --- Testing Activation: AdaptiveErfSwish ---\n",
      "    Seed 1/3 (Actual seed: 42) for AdaptiveErfSwish\n",
      "      Seed 1 Loss: 2.6549, Accuracy: 0.7601\n",
      "    Seed 2/3 (Actual seed: 43) for AdaptiveErfSwish\n",
      "      Seed 2 Loss: 2.2454, Accuracy: 0.7517\n",
      "    Seed 3/3 (Actual seed: 44) for AdaptiveErfSwish\n",
      "      Seed 3 Loss: 2.4698, Accuracy: 0.7617\n",
      "    Results for AdaptiveErfSwish: Mean Loss: 2.4567, Mean Accuracy: 0.7578\n",
      "\n",
      "  --- Testing Activation: ParametricBetaSoftsign ---\n",
      "    Seed 1/3 (Actual seed: 42) for ParametricBetaSoftsign\n",
      "      Seed 1 Loss: nan, Accuracy: 0.1000\n",
      "    Seed 2/3 (Actual seed: 43) for ParametricBetaSoftsign\n",
      "      Seed 2 Loss: nan, Accuracy: 0.1000\n",
      "    Seed 3/3 (Actual seed: 44) for ParametricBetaSoftsign\n",
      "      Seed 3 Loss: nan, Accuracy: 0.1000\n",
      "\n",
      "  --- Testing Activation: ParametricArcSinhGate ---\n",
      "    Seed 1/3 (Actual seed: 42) for ParametricArcSinhGate\n",
      "      Seed 1 Loss: 2.6342, Accuracy: 0.7697\n",
      "    Seed 2/3 (Actual seed: 43) for ParametricArcSinhGate\n",
      "      Seed 2 Loss: 2.0544, Accuracy: 0.7593\n",
      "    Seed 3/3 (Actual seed: 44) for ParametricArcSinhGate\n",
      "      Seed 3 Loss: 1.7489, Accuracy: 0.7507\n",
      "    Results for ParametricArcSinhGate: Mean Loss: 2.1458, Mean Accuracy: 0.7599\n",
      "\n",
      "  --- Testing Activation: GeneralizedAlphaSigmoid ---\n",
      "    Seed 1/3 (Actual seed: 42) for GeneralizedAlphaSigmoid\n",
      "      Seed 1 Loss: nan, Accuracy: 0.1000\n",
      "    Seed 2/3 (Actual seed: 43) for GeneralizedAlphaSigmoid\n",
      "      Seed 2 Loss: nan, Accuracy: 0.1000\n",
      "    Seed 3/3 (Actual seed: 44) for GeneralizedAlphaSigmoid\n",
      "      Seed 3 Loss: 1.1980, Accuracy: 0.5894\n",
      "    Results for GeneralizedAlphaSigmoid: Mean Loss: 1.1980, Mean Accuracy: 0.2631\n",
      "\n",
      "  --- Testing Activation: RiemannianSoftsignActivation ---\n",
      "    Seed 1/3 (Actual seed: 42) for RiemannianSoftsignActivation\n",
      "      Seed 1 Loss: 1.0944, Accuracy: 0.6246\n",
      "    Seed 2/3 (Actual seed: 43) for RiemannianSoftsignActivation\n",
      "      Seed 2 Loss: 1.0671, Accuracy: 0.6356\n",
      "    Seed 3/3 (Actual seed: 44) for RiemannianSoftsignActivation\n",
      "      Seed 3 Loss: 1.1314, Accuracy: 0.6147\n",
      "    Results for RiemannianSoftsignActivation: Mean Loss: 1.0976, Mean Accuracy: 0.6250\n",
      "\n",
      "  --- Testing Activation: QuantumTanhActivation ---\n",
      "    Seed 1/3 (Actual seed: 42) for QuantumTanhActivation\n",
      "      Seed 1 Loss: 1.9791, Accuracy: 0.2709\n",
      "    Seed 2/3 (Actual seed: 43) for QuantumTanhActivation\n",
      "      Seed 2 Loss: 1.9362, Accuracy: 0.2942\n",
      "    Seed 3/3 (Actual seed: 44) for QuantumTanhActivation\n",
      "      Seed 3 Loss: 1.9475, Accuracy: 0.2979\n",
      "    Results for QuantumTanhActivation: Mean Loss: 1.9542, Mean Accuracy: 0.2877\n",
      "\n",
      "  --- Testing Activation: BipolarGaussianArctanActivation ---\n",
      "    Seed 1/3 (Actual seed: 42) for BipolarGaussianArctanActivation\n",
      "      Seed 1 Loss: 2.3058, Accuracy: 0.1139\n",
      "    Seed 2/3 (Actual seed: 43) for BipolarGaussianArctanActivation\n",
      "      Seed 2 Loss: 2.2553, Accuracy: 0.1483\n",
      "    Seed 3/3 (Actual seed: 44) for BipolarGaussianArctanActivation\n",
      "      Seed 3 Loss: 2.3009, Accuracy: 0.1254\n",
      "    Results for BipolarGaussianArctanActivation: Mean Loss: 2.2873, Mean Accuracy: 0.1292\n",
      "\n",
      "  --- Testing Activation: EllipticGaussianActivation ---\n",
      "    Seed 1/3 (Actual seed: 42) for EllipticGaussianActivation\n",
      "      Seed 1 Loss: 2.2913, Accuracy: 0.7066\n",
      "    Seed 2/3 (Actual seed: 43) for EllipticGaussianActivation\n",
      "      Seed 2 Loss: 2.0676, Accuracy: 0.7173\n",
      "    Seed 3/3 (Actual seed: 44) for EllipticGaussianActivation\n",
      "      Seed 3 Loss: 2.4007, Accuracy: 0.7267\n",
      "    Results for EllipticGaussianActivation: Mean Loss: 2.2532, Mean Accuracy: 0.7169\n",
      "\n",
      "  --- Testing Activation: ExpArcTanHarmonicActivation ---\n",
      "    Seed 1/3 (Actual seed: 42) for ExpArcTanHarmonicActivation\n",
      "      Seed 1 Loss: 2.1614, Accuracy: 0.2022\n",
      "    Seed 2/3 (Actual seed: 43) for ExpArcTanHarmonicActivation\n",
      "      Seed 2 Loss: 2.2035, Accuracy: 0.1827\n",
      "    Seed 3/3 (Actual seed: 44) for ExpArcTanHarmonicActivation\n",
      "      Seed 3 Loss: 2.2462, Accuracy: 0.1726\n",
      "    Results for ExpArcTanHarmonicActivation: Mean Loss: 2.2037, Mean Accuracy: 0.1858\n",
      "\n",
      "  --- Testing Activation: ParametricLogish ---\n",
      "    Seed 1/3 (Actual seed: 42) for ParametricLogish\n",
      "      Seed 1 Loss: 2.3430, Accuracy: 0.7725\n",
      "    Seed 2/3 (Actual seed: 43) for ParametricLogish\n",
      "      Seed 2 Loss: 2.1394, Accuracy: 0.7676\n",
      "    Seed 3/3 (Actual seed: 44) for ParametricLogish\n",
      "      Seed 3 Loss: 1.9376, Accuracy: 0.7789\n",
      "    Results for ParametricLogish: Mean Loss: 2.1400, Mean Accuracy: 0.7730\n",
      "\n",
      "  --- Testing Activation: A_ELuC ---\n",
      "    Seed 1/3 (Actual seed: 42) for A_ELuC\n",
      "      Seed 1 Loss: 2.4076, Accuracy: 0.7749\n",
      "    Seed 2/3 (Actual seed: 43) for A_ELuC\n",
      "      Seed 2 Loss: 1.4674, Accuracy: 0.7406\n",
      "    Seed 3/3 (Actual seed: 44) for A_ELuC\n",
      "      Seed 3 Loss: 2.3065, Accuracy: 0.7766\n",
      "    Results for A_ELuC: Mean Loss: 2.0605, Mean Accuracy: 0.7640\n",
      "\n",
      "  --- Testing Activation: OptimA ---\n",
      "    Seed 1/3 (Actual seed: 42) for OptimA\n",
      "      Seed 1 Loss: 1.9338, Accuracy: 0.7590\n",
      "    Seed 2/3 (Actual seed: 43) for OptimA\n",
      "      Seed 2 Loss: 2.4394, Accuracy: 0.7704\n",
      "    Seed 3/3 (Actual seed: 44) for OptimA\n",
      "      Seed 3 Loss: 2.0053, Accuracy: 0.7757\n",
      "    Results for OptimA: Mean Loss: 2.1261, Mean Accuracy: 0.7684\n",
      "\n",
      "  --- Testing Activation: ParametricSmoothStep ---\n",
      "    Seed 1/3 (Actual seed: 42) for ParametricSmoothStep\n",
      "      Seed 1 Loss: 2.2415, Accuracy: 0.7132\n",
      "    Seed 2/3 (Actual seed: 43) for ParametricSmoothStep\n",
      "      Seed 2 Loss: 2.4300, Accuracy: 0.7360\n",
      "    Seed 3/3 (Actual seed: 44) for ParametricSmoothStep\n",
      "      Seed 3 Loss: 2.1692, Accuracy: 0.7377\n",
      "    Results for ParametricSmoothStep: Mean Loss: 2.2802, Mean Accuracy: 0.7290\n",
      "\n",
      "  --- Testing Activation: OptimXTemporal ---\n",
      "    Seed 1/3 (Actual seed: 42) for OptimXTemporal\n",
      "      Seed 1 Loss: 2.0727, Accuracy: 0.7469\n",
      "    Seed 2/3 (Actual seed: 43) for OptimXTemporal\n",
      "      Seed 2 Loss: 1.6119, Accuracy: 0.7417\n",
      "    Seed 3/3 (Actual seed: 44) for OptimXTemporal\n",
      "      Seed 3 Loss: 2.1503, Accuracy: 0.7448\n",
      "    Results for OptimXTemporal: Mean Loss: 1.9450, Mean Accuracy: 0.7445\n",
      "\n",
      "  --- Testing Activation: ExpoSoft ---\n",
      "    Seed 1/3 (Actual seed: 42) for ExpoSoft\n",
      "      Seed 1 Loss: 2.0108, Accuracy: 0.6868\n",
      "    Seed 2/3 (Actual seed: 43) for ExpoSoft\n",
      "      Seed 2 Loss: 1.9690, Accuracy: 0.6263\n",
      "    Seed 3/3 (Actual seed: 44) for ExpoSoft\n",
      "      Seed 3 Loss: 1.6362, Accuracy: 0.6423\n",
      "    Results for ExpoSoft: Mean Loss: 1.8720, Mean Accuracy: 0.6518\n",
      "\n",
      "  --- Testing Activation: UnifiedSineExp ---\n",
      "    Seed 1/3 (Actual seed: 42) for UnifiedSineExp\n",
      "      Seed 1 Loss: 2.2969, Accuracy: 0.1264\n",
      "    Seed 2/3 (Actual seed: 43) for UnifiedSineExp\n",
      "      Seed 2 Loss: 2.2892, Accuracy: 0.5616\n",
      "    Seed 3/3 (Actual seed: 44) for UnifiedSineExp\n",
      "      Seed 3 Loss: 1.4101, Accuracy: 0.4972\n",
      "    Results for UnifiedSineExp: Mean Loss: 1.9987, Mean Accuracy: 0.3951\n",
      "\n",
      "\n",
      "--- Final Comparison Results ---\n",
      "                                 activation  mean_accuracy  std_accuracy  best_accuracy  mean_loss  std_loss            notes\n",
      "18                         ParametricLogish       0.773000      0.004627         0.7789   2.139989  0.165509                 \n",
      "1                                      gelu       0.770333      0.003919         0.7751   2.234624  0.218237                 \n",
      "2                                     swish       0.769733      0.004646         0.7736   2.190673  0.165957                 \n",
      "20                                   OptimA       0.768367      0.006968         0.7757   2.126127  0.223413                 \n",
      "0                                      relu       0.767967      0.005992         0.7725   2.262353  0.304752                 \n",
      "8                 WeibullSoftplusActivation       0.764433      0.006761         0.7712   2.050251  0.227821                 \n",
      "19                                   A_ELuC       0.764033      0.016584         0.7766   2.060515  0.421409                 \n",
      "3                                      mish       0.763833      0.012682         0.7744   2.150477  0.310537                 \n",
      "11                    ParametricArcSinhGate       0.759900      0.007768         0.7697   2.145817  0.367147                 \n",
      "9                          AdaptiveErfSwish       0.757833      0.004386         0.7617   2.456712  0.167402                 \n",
      "6   ParametricGeneralizedGompertzActivation       0.753500      0.007024         0.7615   2.282716  0.225415                 \n",
      "5               AdaptiveHyperbolicLogarithm       0.750233      0.005572         0.7580   1.922518  0.152747                 \n",
      "22                           OptimXTemporal       0.744467      0.002136         0.7469   1.944969  0.237599                 \n",
      "21                     ParametricSmoothStep       0.728967      0.011170         0.7377   2.280226  0.109944                 \n",
      "4                                   sigmoid       0.728933      0.015042         0.7416   1.720320  0.396511                 \n",
      "16               EllipticGaussianActivation       0.716867      0.008212         0.7267   2.253225  0.138608                 \n",
      "23                                 ExpoSoft       0.651800      0.025596         0.6868   1.872000  0.167580                 \n",
      "13             RiemannianSoftsignActivation       0.624967      0.008536         0.6356   1.097608  0.026343                 \n",
      "7                 ComplexHarmonicActivation       0.474067      0.012419         0.4915   1.467045  0.023866                 \n",
      "24                           UnifiedSineExp       0.395067      0.191787         0.5616   1.998717  0.416210                 \n",
      "14                    QuantumTanhActivation       0.287667      0.011952         0.2979   1.954240  0.018141                 \n",
      "12                  GeneralizedAlphaSigmoid       0.263133      0.230705         0.5894   1.198049  0.000000                 \n",
      "17              ExpArcTanHarmonicActivation       0.185833      0.012286         0.2022   2.203728  0.034595                 \n",
      "15          BipolarGaussianArctanActivation       0.129200      0.014298         0.1483   2.287335  0.022747                 \n",
      "10                   ParametricBetaSoftsign            NaN           NaN            NaN        NaN       NaN  All runs failed\n",
      "\n",
      "Results saved to cifar10_activation_comparison_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1458: RuntimeWarning: invalid value encountered in greater\n",
      "  has_large_values = (abs_vals > 1e6).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in less\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n",
      "/usr/local/lib/python3.11/dist-packages/pandas/io/formats/format.py:1459: RuntimeWarning: invalid value encountered in greater\n",
      "  has_small_values = ((abs_vals < 10 ** (-self.digits)) & (abs_vals > 0)).any()\n"
     ]
    }
   ],
   "source": [
    "# --- 7. Main Training and Evaluation Loop ---\n",
    "if __name__ == '__main__':\n",
    "    print(f\"TensorFlow Version: {tf.__version__}\")\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            print(f\"Num GPUs Available: {len(gpus)}, Memory growth enabled.\")\n",
    "            print(\"TensorFlow is using GPU.\")\n",
    "        except RuntimeError as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print(\"TensorFlow is using CPU.\")\n",
    "\n",
    "    # Using a single dataset for this benchmark\n",
    "    dataset_name = \"CIFAR-10\"\n",
    "    (x_train, y_train), (x_test, y_test) = load_and_preprocess_cifar10()\n",
    "    input_shape = x_train.shape[1:]\n",
    "    num_classes = y_train.shape[1]\n",
    "    \n",
    "    standard_activations = ['relu', 'gelu', 'swish', 'mish', 'sigmoid']\n",
    "    # Test standard activations and all found in actix\n",
    "    activations_to_test = standard_activations + list(CUSTOM_ACTIVATIONS_MAP.keys())\n",
    "    \n",
    "    all_results_list = []\n",
    "\n",
    "    print(f\"\\n\\n--- Benchmarking on Dataset: {dataset_name} ---\")\n",
    "\n",
    "    for act_name in activations_to_test:\n",
    "        print(f\"\\n  --- Testing Activation: {act_name} ---\")\n",
    "        losses_for_activation = []\n",
    "        accuracies_for_activation = []\n",
    "        \n",
    "        # Pre-check model build\n",
    "        try:\n",
    "            tf.keras.backend.clear_session()\n",
    "            _reset_layer_name_counters()\n",
    "            _ = create_cifar10_cnn_model(input_shape, act_name, num_classes)\n",
    "        except Exception as e:\n",
    "            print(f\"    ERROR: Could not build model with {act_name}. Error: {e}\")\n",
    "            result_entry = {\n",
    "                'activation': act_name,\n",
    "                'mean_loss': np.nan, 'std_loss': np.nan,\n",
    "                'mean_accuracy': np.nan, 'std_accuracy': np.nan, 'best_accuracy': np.nan,\n",
    "                'notes': f'Failed to build model: {e}'\n",
    "            }\n",
    "            all_results_list.append(result_entry)\n",
    "            continue\n",
    "\n",
    "        for i in range(NUM_SEEDS):\n",
    "            seed = 42 + i\n",
    "            print(f\"    Seed {i+1}/{NUM_SEEDS} (Actual seed: {seed}) for {act_name}\")\n",
    "            tf.keras.utils.set_random_seed(seed)\n",
    "            tf.keras.backend.clear_session()\n",
    "            _reset_layer_name_counters()\n",
    "\n",
    "            model = create_cifar10_cnn_model(input_shape, act_name, num_classes)\n",
    "            \n",
    "            callbacks = [\n",
    "                EarlyStopping(monitor='val_accuracy', patience=PATIENCE, restore_best_weights=True, mode='max', verbose=0),\n",
    "            ]\n",
    "            \n",
    "            try:\n",
    "                history = model.fit(x_train, y_train,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    epochs=EPOCHS,\n",
    "                                    validation_data=(x_test, y_test),\n",
    "                                    callbacks=callbacks,\n",
    "                                    verbose=0) # verbose=0 for a cleaner log\n",
    "                \n",
    "                # evaluate returns [loss, accuracy] as per compiled metrics\n",
    "                eval_results = model.evaluate(x_test, y_test, verbose=0)\n",
    "                loss, accuracy = eval_results[0], eval_results[1]\n",
    "                \n",
    "                losses_for_activation.append(loss)\n",
    "                accuracies_for_activation.append(accuracy)\n",
    "                print(f\"      Seed {i+1} Loss: {loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"      ERROR during training/evaluation for {act_name} with seed {seed}: {e}\")\n",
    "                losses_for_activation.append(np.nan)\n",
    "                accuracies_for_activation.append(np.nan)\n",
    "\n",
    "        # Calculate final statistics across all seeds\n",
    "        if any(not np.isnan(l) for l in losses_for_activation):\n",
    "            mean_loss = np.nanmean(losses_for_activation)\n",
    "            std_loss = np.nanstd(losses_for_activation)\n",
    "\n",
    "            mean_accuracy = np.nanmean(accuracies_for_activation)\n",
    "            std_accuracy = np.nanstd(accuracies_for_activation)\n",
    "            best_accuracy = np.nanmax(accuracies_for_activation)\n",
    "\n",
    "            result_entry = {\n",
    "                'activation': act_name,\n",
    "                'mean_loss': mean_loss, 'std_loss': std_loss,\n",
    "                'mean_accuracy': mean_accuracy, 'std_accuracy': std_accuracy, 'best_accuracy': best_accuracy,\n",
    "                'notes': ''\n",
    "            }\n",
    "            print(f\"    Results for {act_name}: Mean Loss: {mean_loss:.4f}, Mean Accuracy: {mean_accuracy:.4f}\")\n",
    "        else:\n",
    "            result_entry = {\n",
    "                'activation': act_name,\n",
    "                'mean_loss': np.nan, 'std_loss': np.nan,\n",
    "                'mean_accuracy': np.nan, 'std_accuracy': np.nan, 'best_accuracy': np.nan,\n",
    "                'notes': 'All runs failed'\n",
    "            }\n",
    "        all_results_list.append(result_entry)\n",
    "\n",
    "    # --- 8. Output and Saving Results ---\n",
    "    print(\"\\n\\n--- Final Comparison Results ---\")\n",
    "    results_df = pd.DataFrame(all_results_list)\n",
    "    \n",
    "    # Sort by mean accuracy (from best to worst)\n",
    "    results_df = results_df.sort_values(by=['mean_accuracy'], ascending=False)\n",
    "    \n",
    "    pd.set_option('display.max_rows', None)\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.width', 1000)\n",
    "\n",
    "    print(results_df[['activation', 'mean_accuracy', 'std_accuracy', 'best_accuracy', 'mean_loss', 'std_loss', 'notes']])\n",
    "\n",
    "    # Save to CSV\n",
    "    output_filename = \"cifar10_activation_comparison_results.csv\"\n",
    "    results_df.to_csv(output_filename, index=False)\n",
    "    print(f\"\\nResults saved to {output_filename}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 21916.820468,
   "end_time": "2025-06-07T17:58:09.901803",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-06-07T11:52:53.081335",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
